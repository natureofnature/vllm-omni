# Examples

This section contains example code and tutorials for using vLLM-omni.

## Offline Inference

### Qwen2.5-omni

Example of offline inference with Qwen2.5-omni model.

```python
--8<-- "examples/offline_inference/qwen_2_5_omni/end2end.py"
```

More examples are available in the [examples directory](https://github.com/vllm-project/vllm-omni/tree/main/examples) on GitHub.

